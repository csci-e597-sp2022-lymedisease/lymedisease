# -*- coding: utf-8 -*-
"""LymeBerTweet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bMOWIGl-pJNAIgx1HQBK_ze5gxcNOnBj
"""


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# import plotly.figure_factory as ff
# from transformers import TFAutoModel
from transformers import TrainingArguments
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import Trainer
from transformers import TrainingArguments
from datasets import load_metric
from sklearn.metrics import classification_report, precision_score, \
    recall_score, f1_score, accuracy_score, precision_recall_fscore_support


# https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf
from datasets import load_dataset
dataset = load_dataset('csv', data_files={'train': '../data/NLP_train.csv', 'test': '../data/NLP_test.csv'}, encoding = "ISO-8859-1")

tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

def transform_labels(label):

    num = label['keywords_label']
    return {'labels': num}

def tokenize_data(example):
    return tokenizer(example['fixed_text'], padding='max_length')

dataset = dataset.map(tokenize_data, batched=True)

remove_columns = ['id','text','keywords_label','fixed_text']
dataset = dataset.map(transform_labels, remove_columns=remove_columns)

print("dataset:",dataset)

model = AutoModelForSequenceClassification.from_pretrained("vinai/bertweet-base")

temp_ds = dataset['train'].shuffle(seed=111)
train_dataset = temp_ds.select(range(60000))
eval_dataset = temp_ds.select(range(60000, 70000))

print("train_dataset:", train_dataset)

print("eval_dataset:", eval_dataset)

print("dataset['train']:", dataset['train'])

print("type(dataset['train']):", type(dataset['train']))

batch_size=64
epochs=2
training_args = TrainingArguments("test_trainer", num_train_epochs=epochs, 
                  learning_rate=2e-5, per_device_train_batch_size=batch_size, 
                  per_device_eval_batch_size=batch_size, weight_decay=0.01,
                  evaluation_strategy="epoch")

from sklearn.metrics import classification_report, precision_score, \
    recall_score, f1_score, accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

trainer = Trainer(
    model=model, args=training_args, compute_metrics=compute_metrics,
    train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer
)
trainer.train()



pred_output = trainer.predict(eval_dataset)
pred_output.metrics

test_dataset = dataset['test']
pred_output_test = trainer.predict(test_dataset)
pred_output_test.metrics



