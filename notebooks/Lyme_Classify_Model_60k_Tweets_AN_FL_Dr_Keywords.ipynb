{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lyme Classify Model 60k Tweets AN/FL/Dr. Keywords.ipynb","provenance":[],"authorship_tag":"ABX9TyMDrqoSkb6TZq8jjIWcpLdR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"el7zgyeoRveW","executionInfo":{"status":"ok","timestamp":1645333274436,"user_tz":300,"elapsed":171,"user":{"displayName":"Austen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12840155795348996959"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6154a6f-163f-4f9f-cd39-c0fc483fa625"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import re\n","import pandas as pd\n","import numpy as np\n","\n","#for text pre-processing\n","import re, string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","#for model-building\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","\n","# bag of words\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#for word embedding\n","import gensim\n","from gensim.models import Word2Vec"]},{"cell_type":"code","source":["import os\n","import os.path\n","\n","path = \"/content/sample_data/\" \n","os.chdir(path)"],"metadata":{"id":"V3lAh8drXbCi","executionInfo":{"status":"ok","timestamp":1645333278803,"user_tz":300,"elapsed":180,"user":{"displayName":"Austen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12840155795348996959"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# This is the 60k tweets selected from the Carmen processed data\n","df = pd.read_csv('NLP_train.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"Kttv4yWgVns3","executionInfo":{"status":"ok","timestamp":1645334126962,"user_tz":300,"elapsed":1335,"user":{"displayName":"Austen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12840155795348996959"}},"outputId":"68fc0fdc-b9f3-44f8-d08e-94244182be6c"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-2558c291-e910-486c-8bd7-5ef65c98d746\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>author_id</th>\n","      <th>public_metrics</th>\n","      <th>created_at</th>\n","      <th>in_reply_to_user_id</th>\n","      <th>conversation_id</th>\n","      <th>reply_settings</th>\n","      <th>source</th>\n","      <th>referenced_tweets</th>\n","      <th>context_annotations</th>\n","      <th>entities</th>\n","      <th>user</th>\n","      <th>user_name</th>\n","      <th>city</th>\n","      <th>state</th>\n","      <th>county</th>\n","      <th>country</th>\n","      <th>baseline_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>8898041343901696</td>\n","      <td>She was sick last week and now again last nigh...</td>\n","      <td>165742597</td>\n","      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n","      <td>2010-11-28T15:00:33.000Z</td>\n","      <td>NaN</td>\n","      <td>8898041343901696</td>\n","      <td>everyone</td>\n","      <td>Twitter Web Client</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'hashtags': [{'start': 99, 'end': 104, 'tag':...</td>\n","      <td>{'location': 'PA', 'created_at': '2010-07-12T1...</td>\n","      <td>GFfoodsROCK</td>\n","      <td>NaN</td>\n","      <td>Pennsylvania</td>\n","      <td>NaN</td>\n","      <td>United States</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>10423674930733056</td>\n","      <td>Anyone take tindamax for #Lyme DD takes this l...</td>\n","      <td>165742597</td>\n","      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n","      <td>2010-12-02T20:02:52.000Z</td>\n","      <td>NaN</td>\n","      <td>10423674930733056</td>\n","      <td>everyone</td>\n","      <td>Twitter Web Client</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'hashtags': [{'start': 25, 'end': 30, 'tag': ...</td>\n","      <td>{'location': 'PA', 'created_at': '2010-07-12T1...</td>\n","      <td>GFfoodsROCK</td>\n","      <td>NaN</td>\n","      <td>Pennsylvania</td>\n","      <td>NaN</td>\n","      <td>United States</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7647964877</td>\n","      <td>Time for Lyme gives research grant to Dr. Robe...</td>\n","      <td>34282571</td>\n","      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n","      <td>2010-01-11T23:45:24.000Z</td>\n","      <td>NaN</td>\n","      <td>7647964877</td>\n","      <td>everyone</td>\n","      <td>Twitter Web Client</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'annotations': [{'start': 9, 'end': 12, 'prob...</td>\n","      <td>{'username': 'Kulambo', 'public_metrics': {'fo...</td>\n","      <td>Kulambo</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>United States</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>19800587398815744</td>\n","      <td>@IkenCEO @filemot Hv just started novel (set i...</td>\n","      <td>74001901</td>\n","      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n","      <td>2010-12-28T17:03:22.000Z</td>\n","      <td>49366829.0</td>\n","      <td>19789870159372289</td>\n","      <td>everyone</td>\n","      <td>Twitter Web Client</td>\n","      <td>[{'type': 'replied_to', 'id': '197898701593722...</td>\n","      <td>NaN</td>\n","      <td>{'annotations': [{'start': 48, 'end': 57, 'pro...</td>\n","      <td>{'name': 'Rochelle Almeida', 'location': 'Sout...</td>\n","      <td>southportgal</td>\n","      <td>NaN</td>\n","      <td>Connecticut</td>\n","      <td>NaN</td>\n","      <td>United States</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>12513156315</td>\n","      <td>Dog Blogged:: Old Lyme Disease - Oh yeah, I've...</td>\n","      <td>20862522</td>\n","      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n","      <td>2010-04-20T12:26:57.000Z</td>\n","      <td>NaN</td>\n","      <td>12513156315</td>\n","      <td>everyone</td>\n","      <td>Twitter Web Client</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'location': 'CT USA', 'created_at': '2009-02-...</td>\n","      <td>ScouttheDog</td>\n","      <td>NaN</td>\n","      <td>Connecticut</td>\n","      <td>NaN</td>\n","      <td>United States</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2558c291-e910-486c-8bd7-5ef65c98d746')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2558c291-e910-486c-8bd7-5ef65c98d746 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2558c291-e910-486c-8bd7-5ef65c98d746');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Unnamed: 0                 id  ...        country  baseline_label\n","0           0   8898041343901696  ...  United States            True\n","1           1  10423674930733056  ...  United States            True\n","2           2         7647964877  ...  United States           False\n","3           3  19800587398815744  ...  United States           False\n","4           4        12513156315  ...  United States            True\n","\n","[5 rows x 20 columns]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Source/reference- https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e\n","\n","#convert to lowercase, strip and remove punctuations\n","def preprocess(text):\n","    text = text.lower() \n","    text=text.strip()  \n","    text=re.compile('<.*?>').sub('', text) \n","    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n","    text = re.sub('\\s+', ' ', text)  \n","    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n","    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n","    text = re.sub(r'\\d',' ',text) \n","    text = re.sub(r'\\s+',' ',text) \n","    return text\n","\n"," \n","# STOPWORD REMOVAL\n","def stopword(string):\n","    a= [i for i in string.split() if i not in stopwords.words('english')]\n","    return ' '.join(a)\n","    \n","#LEMMATIZATION\n","# Initialize the lemmatizer\n","wl = WordNetLemmatizer()\n"," \n","# This is a helper function to map NTLK position tags\n","def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","# Tokenize the sentence\n","def lemmatizer(string):\n","    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n","    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n","    return \" \".join(a)\n","\n","def finalpreprocess(string):\n","    return lemmatizer(stopword(preprocess(string)))\n","    \n","df['clean_text'] = df['text'].apply(lambda x: finalpreprocess(x))\n","df.head()"],"metadata":{"id":"Y_NRXoSdzoSc","executionInfo":{"status":"ok","timestamp":1645334129676,"user_tz":300,"elapsed":144,"user":{"displayName":"Austen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12840155795348996959"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Create baseline dataframe with tweets randomly labelled with 0 (non-Lyme) and 1 (Lyme)\n","data = np.random.randint(0,2,size=len(df))\n","df['random_label'] = data\n","df.head()"],"metadata":{"id":"IROYmRSpJA1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Source/reference- https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e\n","\n","#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n","X_train, X_test, y_train, y_test = train_test_split(df['clean_text'],df['baseline_label'], test_size=0.2, shuffle=True)\n","\n","#Word2Vec\n","# Word2Vec runs on tokenized sentences\n","X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n","X_test_tok= [nltk.word_tokenize(i) for i in X_test]\n","\n","#Tf-Idf\n","tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n","X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n","X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","#building Word2Vec model\n","class MeanEmbeddingVectorizer(object):\n","    def __init__(self, word2vec):\n","        self.word2vec = word2vec\n","        # if a text is empty we should return a vector of zeros\n","        # with the same dimensionality as all the other vectors\n","        self.dim = len(next(iter(word2vec.values())))\n","    def fit(self, X, y):\n","        return self\n","    def transform(self, X):\n","        return np.array([\n","            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n","            for words in X\n","        ])\n","\n","df['clean_text_tok']=[nltk.word_tokenize(i) for i in df['clean_text']]\n","model = Word2Vec(df['clean_text_tok'],min_count=1)\n","w2v = dict(zip(model.wv.index2word, model.wv.syn0))     \n","modelw = MeanEmbeddingVectorizer(w2v)\n","\n","# converting text to numerical data using Word2Vec\n","X_train_vectors_w2v = modelw.transform(X_train_tok)\n","X_val_vectors_w2v = modelw.transform(X_test_tok)"],"metadata":{"id":"k6MQm4iGzoYR","executionInfo":{"status":"ok","timestamp":1645334392401,"user_tz":300,"elapsed":7619,"user":{"displayName":"Austen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12840155795348996959"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n","lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n","lr_tfidf.fit(X_train_vectors_tfidf, y_train)\n","\n","#Predict y value for test dataset\n","y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n","y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n","print(classification_report(y_test,y_predict))\n","print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n"," \n","fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n","roc_auc = auc(fpr, tpr)\n","print('AUC:', roc_auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"preyv0Nk6ypm","executionInfo":{"status":"ok","timestamp":1645334429632,"user_tz":300,"elapsed":742,"user":{"displayName":"Austen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12840155795348996959"}},"outputId":"6592962d-0929-4a4e-8cc1-e3f2d96ddab3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       False       0.92      0.97      0.95      3462\n","        True       0.99      0.97      0.98      8538\n","\n","    accuracy                           0.97     12000\n","   macro avg       0.96      0.97      0.96     12000\n","weighted avg       0.97      0.97      0.97     12000\n","\n","Confusion Matrix: [[3360  102]\n"," [ 276 8262]]\n","AUC: 0.992621713320502\n"]}]},{"cell_type":"code","source":["# Test Model with 6000 Ground Truth Labeled Tweets with Geo Tags"],"metadata":{"id":"HSOiePIpl0_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use Model to Classify 27994 Tweets with Geo Tags"],"metadata":{"id":"Lh2wWzfVmFPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Correlate the 27994 Classified Tweets to CDC Data"],"metadata":{"id":"RdEEBa9NmjTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plan B: Isolate Tweets that Confirm the person has Lyme or No Lyme. Then repeat the prior steps"],"metadata":{"id":"EBgNhHBPnOZA"},"execution_count":null,"outputs":[]}]}